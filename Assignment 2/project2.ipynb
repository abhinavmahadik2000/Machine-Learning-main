{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d003bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227c160f",
   "metadata": {},
   "source": [
    "## Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0870424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self, i, errors):\n",
    "        self.layers = i\n",
    "        self.errs = errors\n",
    "        \n",
    "    \n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self):\n",
    "        self.grad = self.errs.backward()\n",
    "        i = len(self.layers) - 1\n",
    "        \n",
    "        while i >= 0:\n",
    "            self.grad = self.layers[i].backward(self.grad)\n",
    "            i -= 1\n",
    "        \n",
    "    def _errs(self, X, y):\n",
    "        return self.errs.forward(self.forward(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8f5b97",
   "metadata": {},
   "source": [
    "## Linear Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c46a61bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, X, y):\n",
    "        self.w = np.random.rand(X, y) * 0.001\n",
    "        self.b = np.zeros(y)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.y = (X @ self.w) + self.b\n",
    "        \n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, grad_X):\n",
    "        self.dw = np.matmulmatmul(self.X[:,:,None], gradient_X[:,None,:]).mean(axis=0)\n",
    "        self.db = grad_X.mean(axis=0)\n",
    "        \n",
    "        return grad_X @ self.w.T\n",
    "    \n",
    "    def update_w_b(self, lr = 0.001):\n",
    "        self.w -= lr * self.dw\n",
    "        self.b -= lr * self.db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e27bad",
   "metadata": {},
   "source": [
    "## Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "242a27c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.y = 1.0 / (1.0 + np.exp(-X))\n",
    "        \n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, grad_X):\n",
    "        return self.y * (1 - self.y) * grad_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032062f5",
   "metadata": {},
   "source": [
    "## Hyperbolic Tangent Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9a56cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperbolic_Tangent(Layer):\n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.y = np.tanh(X)\n",
    "        \n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, grad_X):\n",
    "        return (1 - np.square(self.outputs)) * gradient_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dad81b",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2de6566",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def __init__(self):\n",
    "        self.num = None\n",
    "        self.dim = None\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.num = np.exp(X - np.max(X))\n",
    "        \n",
    "        return self.num / np.sum(self.num, axis=0, keepdims=True)\n",
    "    \n",
    "    def backward(self, probs, bp_err):\n",
    "        self.dim = probs.shape[1]\n",
    "        self.y = np.empty(probs.shape)\n",
    "        \n",
    "        for j in range(self.dim):\n",
    "            d_prob_over_xj = - (probs * probs[:,[j]])  # i.e. prob_k * prob_j, no matter k==j or not\n",
    "            d_prob_over_xj[:,j] += probs[:,j]   # i.e. when k==j, +prob_j\n",
    "            self.y[:,j] = np.sum(bp_err * d_prob_over_xj, axis=1)\n",
    "        return self.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de670ef0",
   "metadata": {},
   "source": [
    "## Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a343cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cross_Entropy_Loss(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, y):\n",
    "        return -y * np.log(self.y)\n",
    "\n",
    "    def backward(self, y):\n",
    "        return y - self.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ed489",
   "metadata": {},
   "source": [
    "## Sequential Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68baf527",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Layer):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        \n",
    "        self.layers = [Linear(X, y), Logistic_Sigmoid(), Linear(X, y), Logistic_Sigmoid()]\n",
    "        self.model = Layer(self.layers, Cross_Entropy_Loss())\n",
    "        \n",
    "    def test(self, X):\n",
    "        y = self.model.forward(X)\n",
    "        return 1 if (np.squeeze(y) > 0.5).all() else 0\n",
    "    \n",
    "    def train(self, ittr = 100):\n",
    "        for i in range(ittr):\n",
    "            err = self.model._errs(self.X, self.y).sum()\n",
    "            \n",
    "            print(f\"Loss in {i}th epoch: {err}\")\n",
    "            \n",
    "            self.model.backward()\n",
    "            \n",
    "            for layer in self.model.layers:\n",
    "                if isinstance(layer, Linear):\n",
    "                    layer.update_w_b()\n",
    "    \n",
    "\n",
    "    def get_w(self):\n",
    "        self.w = []\n",
    "        self.b = []\n",
    "        \n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, Linear):\n",
    "                self.w.append(layer.w)\n",
    "                self.b.append(layer.b)\n",
    "        return self.w\n",
    "\n",
    "    def s_model(self, f=\"model.w\"):\n",
    "        model_file = open(f, mode=\"wb\")\n",
    "        pickle.dump(self.get_w(), model_file)\n",
    "        model_file.close()\n",
    "\n",
    "    def l_model(self, f=\"model.w\"):\n",
    "        model_file = open(f, mode=\"rb\")\n",
    "        w = pickle.load(model_file)\n",
    "        model_file.close()\n",
    "        return w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cse6363-ml)",
   "language": "python",
   "name": "cse6363-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
